# 自然语言处理相关

- word2vec原理
- glove原理？模型推导？
    - 思路是将全局词-词共现矩阵进行分解，训练得到词向量
    - 目标函数： $J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}$
    - $X_{ij}$表示单词j出现在单词i上下文中的次数
- fasttext原理
    - 模型框架类似于Word2vec的CBOW,但是也有区别：
        - 输入：word2vec用的是上下文单词的one-hot编码；fasttext用的是一个sentence的单词向量同时包括subword, 字符级别n-gran向量
        - 中间层：word2vec使用的是求和；fasttext使用的是平均；
        - 输出：cbow输出目标词汇；fasttext输出文本分类标签
    - 速度快，使用层次softmax，对oov友好 
    - 有监督的文本分裂，无监督的词向量学习
- glove和word2vec区别：
    -  g是基于全局语料的，w是基于滑动窗口局部语料的；因此w可以进行在线学习，而g则需要固定语料信息
    -  目标函数不一样
    -  g更快，对于高频词的处理更有效
- LDA的词表示和word2vec的词表示有什么区别？
    - LDA出来的词向量不如word2vec的精细，一些任务无法完成
    - 训练过程LDA使用的是狄利克雷先验分布，属于生成式模型；word2vec使用的是语言模型，属于判别式模型
    - 
- 分词的原理
    - 最大正向匹配，最大逆向匹配，双向匹配
    - n-gram
    - BMES模型（HMM，CRF）
- 字典树的优缺点？手写字典树？
    - 优点：查询速度快，可以做前缀比较
    - 缺点：内存消耗大
- 输入补全可以由那种数据结构来做？
    - 字典树 
- 为什么正则化处理中bias不需要正则
    - bias影响的是结果的偏差，对输入没有放大缩小的作用，而正则化要降低方差（提高模型泛化能力），所以不需要正则化bias
- 反卷积与反池化
    - 反卷积：
    - 反池化：
- attention机制原理
- transformer结构图以及原理
- BLEU原理及代码
- dropout train和test的区别
    - train过程：输出会依一定概率p让输入为0
    - test过程： 需要乘上概率p
- BN  train和test的区别？
    - train： 有batch
    - test：只有单个样本，所以方差和均值使用之前训练阶段每个batch保存下来的平均
- 正负样本不平衡问题？
    -  过采样或欠采样
    -  组合/集成方法：
    -  正负样本设置惩罚权重
    -  focal loss
- BN前向传播公式，反向求导公式？
    -  对一批数据进行归一化，沿着特征方向：$B N\left(x_{i}\right)=\alpha \times \frac{x_{i}-\mu_{b}}{\sqrt{\sigma_{B}^{2}+\epsilon}}+\beta$
    -  其中两个参数alpha和beta是调节参数，可学习的，防止网络表达能力下降
    -  https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
- BN为什么不适用与RNN？
    - RNN是变长的，基于timestep计算，如果使用BN，需要保存每个timestep下batch的均值和方差，效率很低 
- 加了dropout后，神经网络的BP阶段有什么改变，求导公式要怎么改
    - 需要乘上mask向量，因为在前向传播过程中失活的神经元在反向传播时应该也是要不起作用的
- 试分析为什么不能在循环神经网络中的循环连接上直接应用丢弃法？
    - 若放在循环链接，则信息将会因循环进行而逐渐丢失
    - rnn的dropout只能设置在输入输出层上
- 在LSTM中，隐藏层神经元数量与参数数量之间的关系？
    -  对照LSTM中gate过程的公式可算出：num=4*（O*(I+H)+O）
- CNN卷积的反向传播过程
- 胶囊网络？
    - CNN对物体之间的空间关系识别能力不强
    - CNN对物体旋转之后识别能力不强
    - 胶囊是一个包含多个神经元的载体，每个神经元表示了图像中出现的特定实体的各种属性
    - https://www.tinymind.cn/articles/61
- 转置卷积？
    - 一般卷积：$\mathbf{z}=C \mathbf{x}$
    - 转置卷积: $\mathbf{x}=C^{\mathrm{T}} \mathbf{z}$
- 微步卷积？
    - 步长s<1的转置卷积 
    - 如果卷积操作的步长s>1，希望其对应的转置卷积的步长为1/s，需要在输入特征之间插入s-1个0来使得其移动的速度变慢
- 空洞卷积？存在什么问题？
    - 通过间隔地对输入进行卷积操作以增加输出单元的感受野
- 1*1卷积核的作用？
    - 降维
    - 升维
    - 增加非线性
- TensorFlow训练模型的整个流程？
- 梯度消失是否可以通过增加学习率来缓解？
- 卷积层神经元数量计算？
    - 假设卷积层的输入神经元个数为n，卷积大小为m，步长（stride）为s，输入神经元两端各填补p 个零（zero padding），则神经元数量为`$(n-m+2 p) / s+1$`
- CNN的反向传播？
- 什么样的任务适合用深度学习，什么样的问题不适合？
    - 不适合：
        - 小样本
        - 低维数据，大样本量，不如ensemble
        - 
- NLP中的CNN的max pooling
    - 能减少模型参数数量，有利于缓解过拟合
    - 可以把变长的输入X整理成固定长度的输入
    - 丢失最大强度特征的位置信息
    - 有时候强特征出现多次
    - 改进：k-max pooling和chunk-max pooling
- LSTM中为什么选择tanh？
    - https://www.zhihu.com/question/61265076
- 算法的错误样例分析方法？（即预测与真实标签不符）
- warmup策略为什么有效？
    -  warmup是指在训练初始阶段使用较小的学习率来启动，接着切换到大学效率而后进行常见的decay训练
    -  有助于缓解模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳
    -  有助于保持模型深层的稳定性

