# 深度学习相关

本目录主要整理深度学习相关面试知识点。

- 深度学习神经网络为什么不用牛顿法而使用梯度下降？
    - 数据量大： 牛顿法所需统计量（梯度Hessian矩阵等）不可能每一次迭代都使用全部样本
    - 维度高：参数多，导致hessian矩阵巨大
    - 非凸性：牛顿法会受限于鞍点。sgd一定会收敛，但牛顿法不一定会收敛
    - ...
- Batch-norm层的作用？
    - 使网络中每层输入数据的分布相对稳定，加速模型学习速度
    - 使模型对网络中的参数不那么敏感，网络学习更稳定
    - 缓解梯度消失问题
    - 具有一定的正则化效果
    - ...
- 残差网络的作用？
    - 解决深度网络的退化问题
    - 解决梯度弥散问题
    - ...
- 网络初始化有哪些方式，他们的公式 初始化过程？
- 优化方法 SGD、Adam算法过程 动量算法过程
- 

