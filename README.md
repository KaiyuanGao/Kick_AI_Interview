# Kick_AI_Interview
本项目为[Kick_Algorithm](https://github.com/KaiyuanGao/Kick_Algorithm)同源项目，旨在为正在准备AI算法相关面试的同学提供尽可能的帮助。

整理了面试中高频问题，并提供个人回答。能力有限，有错误的欢迎在Issue指出，另外内容肯定不全面，希望有精力有意愿的同学一起贡献整理。

## 目录
:melon:传统机器学习算法  
:apple: 深度学习算法  
:strawberry: NLP相关  
:cherries: Python相关  
:banana: 手撕代码&场景题  

---
## ==传统机器学习算法！！！==
- 常见的loss函数：
    - 平方损失函数（最小二乘法）
        - `$L(Y, f(X))=(Y-f(X))^{2}$`
    - 对数损失函数（方便极大似然估计）
        - `$J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]$` 
    - 指数损失函数（Adaboost）
        -  `$L(Y, f(X))=\frac{1}{n} \sum_{i=1}^{n} e^{-Y_{i} f\left(X_{i}\right)}$`
    -  Hinge损失函数（SVM）
        -   `$L(Y)=\max (0,1-t Y)$`
    -  0-1损失
        -   `$L(Y, f(X))=\left\{\begin{array}{l}{1, Y \neq f(X)} \\ {0, Y=f(X)}\end{array}\right.$`
- 常见的优化算法：
    - https://blog.csdn.net/Kaiyuan_sjtu/article/details/85126721 
- 常见的激活函数：
    - sigmoid/tanh/relu/leaky relu/maxout/softmax
- 常用的评价指标：
    - precision（精确性、查准率）： 预测出的正确正样本数比上所有预测为正样本数
    - recall（召回率、查全率）:预测出的正确正样本数比上所有实际正样本数
    - F1: precision和recall的调和均值
    - ROC（Receiver Operating Characteristic）：
        - 横坐标伪正类率（False Positive Rate），预测为正但实际为负的样本占所有负样本的比例；
        - 纵坐标真正类率（True Positive Rate），预测为正且实际为正的样本占所有正样本的比例；
    - AUC：ROC曲线下的面积
- 常用的距离度量方式
    - 欧式距离：`$d_{a b}=\sqrt{\left(x_{1}-y_{1}\right)^{2}+\left(x_{2}-y_{2}\right)^{2}}$`
    - 曼哈顿距离：`$d_{a b}=\left|x_{1}-y_{1}\right|+\left|x_{2}-y_{2}\right|$`
    - 余弦相似度：`$\cos \theta=\frac{a^{*} b}{|a|^{*}|b|}=\frac{x_{1} * y_{1}+x_{2} * y_{2}}{\sqrt{x_{1}^{2}+x_{2}^{2}} * \sqrt{y_{1}^{2}+y_{2}^{2}}}$`
    - 切比雪夫距离：各对应坐标数值差的最大值，如平面两个向量`$a=(x_{1}, y_{1}), b=(x_{2}, y_{2})$`，其 `$d_{a b}=\max \left\{\left|x_{1}-x_{2}\right|,\left|y_{1}-y_{2}\right|\right\}$`
    - 汉明距离：字符串对应位置的不同字符个数
    - 编辑距离：两个字符串之间由一个转成另一个所需的最少编辑操作次数
    - Person相关系数：反应两个变量的线性相关性。
    - K-L散度：相对熵，`$D(P \| Q)=\sum_{i=1}^{n} P(i) \log \frac{P(i)}{Q(i)}$`


##### 线性回归和LR
- 对于LR模型，特征有<x1,x2,...,xn> 如果手误把第一个特征又加了一次变成了n+1个特征<x1,x2,...,xn,x1>请问会有什么影响？如果是加了一个噪声特征呢？
    -  冗余特征过多，训练过程越慢
    -  会改变模型参数值，使得模型不稳定。极端假设所有特征均重复，则LR模型参数w会变为原来的1/2
- 逻辑回归损失函数为什么用极大似然？
    - 用最小二乘法目标函数就是平方损失的话是非凸函数，会有很多局部最优点 
    - 用最大似然估计目标函数就是对数似然函数
- 回归问题的损失函数都有哪些？从哪些角度设计一个合理的损失函数？
    - 绝对值损失，平方误差损失，huber损失等
    - https://www.zhihu.com/question/68390722/answer/266034620
- 逻辑回归VS线性回归？
    - 都属于广义线性模型
    - 一个回归问题一个分类问题


##### gbdt、xgboost相关
- xgboost和GBDT区别：
    - 基学习模型不同：GBDT使用的是CART，而xgb还支持线性分类器
    - 优化方案不同：gbdt使用一阶导，xgb使用二阶泰勒展开
    - xgb支持自定义代价函数，但必须二阶可导
    - xgb加入了正则化项
    - 列采样
    - xgb支持缺失值的处理，会自动学习出分裂方向
    - xgb支持并行，不是tree粒度的并行而是特征粒度的并行。提前将数据进行排序存在block结构中，后面迭代直接使用减少计算量。
    - 近似直方图算法，用于高效的生成候选的分割点
    - xgb支持交叉验证，方便选取合最优参数，early stop
- xgb如何缓解过拟合？
    - 正则化：对树中叶子节点的数目L1正则，对叶子节点分数L2正则
    - 列抽样：即特征抽样
    - shrinkage：类似学习率，在学习出每一颗树之后对这个树增加一个权重参数，减小其影响力，为后续的树提供空间去优化模型
- xgb如何调参？
- xgb的特征重要性是怎么计算的？
    - 某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和
- adaboost流程？权重更新公式？
    - 加法模型+指数损失+前向分布算法的二类分类学习算法 
    - 流程：
        - 初始化权重分布
        - 计算基分类器在训练集上的分类误差率：`$e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)$`
        - 计算该分类器的对应系数：`$\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$` 
        - 更新训练集的权重分布：`$w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)$`
        - 重复2-4过程直至效果比较好
        - 将所有基分类器加权求和
- gbdt流程？
    - 加法模型+负梯度函数+前向分布算法 
    - 流程：
        - 初始化基学习器：`$f_{0}(x)=\arg \min _{c} \sum_{i=1}^{N} L\left(y_{i}, c\right)$`
        - 对迭代轮数1,2,...,M:
            - 对所有样本，计算负梯度：`$r_{i m}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f=f_{m-1}}$` 
            - 对`$r_{i m}$`拟合一个回归树，得到第m棵树的叶节点区域`$R_{m j}$`,j=1,2,...,J
            - 对j=1,2,...,J：计算`$c_{m j}=\arg \min _{c} \sum_{x_{i} \in R_{m j}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+c\right)$`
            - 更新回归树：`$f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)$`
        - 得到最终回归树：`$\hat{f}(x)=f_{M}(x)=\sum_{m=1}^{M} \sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)$` 
- xgb损失函数？推导？
    -  `$\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)$`，其中等式右侧第二项为正则化项：`$\Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}$`
    -  损失函数的优化：
        -  `$\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)$`
        -  `$\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)$`
        -  `$\tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$`
        -  `$w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}$`
- xgb节点分裂规则？
    - `$G=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{\left(H_{L}+H_{R}\right)+\lambda}\right]-\gamma$`
- xgb如何处理稀疏值？
    -  会分别计算将该样本分到左右子树两种情况的增益，选择增益大的进行分裂。
    -  在测试时，如果出现缺失值，默认右子树
- 为什么gbdt不适合处理sparse特征？
    - 对于高维稀疏数据，每棵树在进行分裂时，金辉选取一个特征，大部分特征是不会被选到的
- gbdt和lr的区别？
    -  都是监督学习，判别式模型
    -  一个线性，一个非线性
    -  lr可以选择一阶导数或者二阶优化，gbdt只有一阶
    -  lr适合处理稀疏数据，gbdt适合稠密特征
    -  
- ==xgb的VC维？==
- 为什么xgb加了二阶梯度会更好？
    - [xgboost是用二阶泰勒展开的优势在哪？](https://www.zhihu.com/question/61374305)
    - 为什么引入二阶泰勒展开？统一损失函数求导的形式以支持自定义损失函数
    - 为什么更好？类比牛顿法和SGD，二阶求导信息量更大，训练过程收敛更快更准确。
- ==lgb对比xgb和原始gbdt的优缺点是什么==
- xgb和LR关于特征的处理有什么区别？
    - LR一般是离散值，而xgb可以连续
    - LR各个特征独立，xgb可以特征组合


##### RF相关
- RF变量重要性排序原理？
    - 平均不纯度的减少：对于每棵树，按照不纯度（gini/information entropy等)给特征排序，然后整个森林取平均
    - 平均准确率的减少：测量每种特征对模型预测准确率的影响


##### kmeans相关
- KNN和kmeans的区别？
    - KNN是有监督学习（分类/回归），kmeans是无监督学习（聚类）
    - k的含义：KNN中k表示找到距离样本X最近的k个点；kmeans表示人为定义将数据分为k个类别
- kmeans的基本流程：
    - 选取k个类的初始中心点
    - 对所有点进行计算距离划分到最近的类
    - 重新计算每一类的中心点
    - 重新回到上述2，如果中心点没变则输出类别
- k值的选取？
    - 先验知识
    - Elbow method
- 如何选取初始中心点？
    - https://www.cnblogs.com/pinard/p/6164214.html   
- kmeans时间复杂度和空间复杂度？
    -  时间：O(NlogN)
    -  空间： O(K*(M+N))

##### EM HMM CRF相关
- EM算法推导？jensen不等式确定的下界？
    - EM算法就是含有隐变量的概率模型参数的极大似然估计
    - ![](http://ww1.sinaimg.cn/large/afd47e42ly1g26n7hu2r0j20f60pegoo.jpg)
    - Jensen不等式：
        - 对于凸函数：`$E[f(X)] \geq f(E[X])$`
        - 对于凹函数：`$E[f(X)] \leq f(E[X])$`
    - EM算法是收敛的，但是不能保证收敛到全局最优 
    - 对初始值的选取敏感
- HMM和CRF的区别？https://www.zhihu.com/question/53458773
    - 两者都可以用于序列模型
    - CRF是无向图，HMM是有向图
    - CRF是判别式模型，HMM是生成式模型
    - CRF没有假设，HMM有马尔科夫假设
    - CRF可以全局最优，HMM可能局部最优
- CRF模型优化目标，怎么训练的？
    - CRF的三个问题以及解决思路
- HMM做了哪些独立性假设？
    -  有限历史假设：即当前状态仅仅与前一个状态有关
    -  输出独立假设：即输出状态仅仅与当前的隐状态有关
    -  齐次性假设：状态与时间无关
- viterbi算法原理
    - 动态规划求最大路径

##### 决策树相关
- 信息增益、信息增益比、基尼系数的公式和原理
    - 信息增益：g=H(D)-H(D|A), 在特征选择时偏向于取值较多的特征,对应ID3算法，该算法只有树的生成，容易过拟合；
    - 信息增益率：`$g_{\mathrm{g}}(D, A)=\frac{g(D, A)}{H_{A}(D)} \quad, \quad \mathrm{H}_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{\mathrm{i}}\right|}{|D|} \log _{2} \frac{\left|D_{\mathrm{i}}\right|}{|D|}$`
    - gini指数：`$\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$`
- ==决策树的VC维==
    - VC维是描述模型复杂度的，模型假设空间越大，vc维越高
    - http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/
    - VC = 节点数+1
- 决策树怎么做特征离散化？
    - 可以采用二分法对连续属性离散化：`$T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leq i \leq n-1\right\}$`
- 决策树的缺失值怎么处理？
    - 对于建树节点分裂过程缺失：对特征计算非缺失样本的熵然后乘上权重（非缺失样本占比）就是该特征最终的熵 
    - 对于建树完成训练时缺失某个特征：将样本分配到每颗分裂出的子树中，然后乘上落入该子树的概率（即该子树中样本比上总样本）
    - 对于预测过程：确定额的划分
- ==CART决策树的剪枝？==
    - https://www.zhihu.com/question/22697086
- CART回归树是怎么做节点划分的？
    - 采用启发式的方法 
- CART为什么采用gini指数作为特征划分标准？
    - 信息增益(比)是基于信息论为熵模型的，会涉及大量的对数运算。而基尼系数和熵之半的曲线非常接近，都可以近似代表分类误差率


##### SVM相关
- SVM推导
- SVM损失函数？
    - hinge：`$L(y)=\max (0,1-t \cdot y)$`
    - 表示当样本点被分类正确且函数间隔大于1时，损失为0；否则损失为`$1-t \cdot y$`
- 为什么要使用hinge loss？
    - 只考虑支持向量的影响
- SVR原理
- 核函数原理、哪些地方引入、如何选择？
    - `$\mathrm{K}(\mathrm{x}, \mathrm{z})=<\Phi(\mathrm{x}), \Phi(\mathrm{Z})>$`
    - https://www.zhihu.com/question/24627666
    - 核函数的作用就是一个从低维到高维的映射
    - 线性：`$K\left(v_{1}, v_{2}\right)=<v_{1}, v_{2}>$`
    - 多项式：`$K\left(v_{1}, v_{2}\right)=\left(\gamma<v_{1}, v_{2}>+c\right)^{n}$`
    - RBF：`$K\left(v_{1}, v_{2}\right)=\exp \left(-\gamma\left\|v_{1}-v_{2}\right\|^{2}\right)$`
    - sigmoid：`$K\left(v_{1}, v_{2}\right)=\tanh \left(\gamma<v_{1}, v_{2}>+c\right)$`
    - 如果特征维数很大（跟样本数量差不多），优先选用线性；如果特征数量小，样本数量一般，选用高斯核；如果特征数量小，样本数量很大，手工添加一些feature变成第一种情况。
- 为什么需要转成对偶形式？
    - 原问题是一个凸二次规划问题   
    - 优化了复杂度。由求特征向量w转化为求比例系数
    - 可以方便引出核函数
- 线性回归的梯度下降和牛顿法求解公式的推导
- 最速下降法和共轭梯度法 wolfe条件 最速下降法和共轭梯度法的收敛速度如何判断
    - 最速下降法即梯度下降：一阶信息
    - 共轭梯度法：介于最速下降和牛顿法之间的一种优化方法，仅需要一阶导数信息（方向限制在初始点的共轭区间内），但收敛速度较快，同时避免了牛顿法求Hessian矩阵的缺点
    - wolfe条件：跟line search有关
- LDA中有哪些分布？定义？什么是共轭分布？
    - 共轭分布：在贝叶斯统计中，如果先验分布和后验分布属于同一类分布，则成这俩为共轭分布
    - LDA过程：
        - 从狄利克雷分布D(a)中采样生成文档的主题分布`$\theta_{i}$`
        - 从主题`$\theta_{i}$`的多项式分布中采样生成文档第j个词的主题`$z_{i, j}$`
        - 从狄利克雷分布D(b)中采样生成主题`$z_{i, j}$`对应的词语分布`$\phi_{\tilde{z}_{i, j}}$`
        - 从词语的多项式分布中采样生成最终词语`$w_{i, j}$`
- k折交叉验证中k取值？
    - 从偏差和方差角度回答： 
        - 当k取值很小时，比如k=2，此时模型训练数据较少，不容易拟合正确，所以偏差较高，方差较低
        - 当k取值较大时，比如k=n，此时相当于所有数据都用于训练，容易过拟合，所以偏差低，方差高
    - 论文给出k值参考公式：k=log(N)，N为样本总数
- KKT条件？（L为拉格朗日函数，g(x), h(x)为约束函数）
```math
\left\{\begin{array}{l}{\nabla_{x} L=0} \\ {\mu g(x)=0} \\ {h(x)=0} \\ {g(x) \leq 0} \\ {\lambda \neq 0} \\ {\mu \geq 0}\end{array}\right.
```

##### 降维算法相关
- 常见降维方法：L1，PCA, LDA t-SNE
- 什么是主成分？
- PCA是一种无监督的降维方法，为了让映射后的样本具有最大的发散性（即尽可能少的重叠，保留原有信息）；
- LDA是一种有监督的降维方法，为了让映射后的样本具有最好的分类性能（即类内方差最小，类间方差最大）
- 局部线性嵌入(LLE)是一种非线性降维算法，能够使降维后的数据较好地保持原有流形结构。

##### 各种ML模型比较
- 逻辑回归VS线性回归？
    - 都属于广义线性模型
    - 一个回归问题一个分类问题
- SVM和LR逻辑回归？
    - 都属于线性分类算法，判别模型，监督学习
    - 损失函数不同
    - 确定决策边界时考虑的训练点不同
    - SVM有核函数，LR虽然也可以用但是一般不适用
    - SVM自带正则项

##### 其他
- 谈谈牛顿法？牛顿法如何优化的？
    - 牛顿法迭代公式：`$x_{k+1}=x_{k}-\frac{f^{\prime}\left(x_{k}\right)}{f^{\prime \prime}\left(x_{k}\right)}$`,优点是二阶导收敛速度快，但是需要就算hessian矩阵的逆，计算复杂
    - 优化：拟牛顿法，使用正定矩阵来近似Hessian矩阵的逆
- 交叉熵损失函数公式？怎么推导得到的？
    - 公式： `$L=\sum_{i=1}^{N} y^{(i)} \log \hat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)$`
    - 推导：
        - `$P(y=1 | x)= \hat{y}$`
        - `$P(y=0 | x)=1- \hat{y}$`
        - `$P(y|x)=\hat{y}^{y} \cdot(1-\hat{y})^{1-y}$`
        - log化：`$\log P(y | x)=\log \left(\hat{y}^{y} \cdot(1-\hat{y})^{1-y}\right)=y \log \hat{y}+(1-y) \log (1-\hat{y})$`
        - 求log化的最大值，前面加个负号就是求最小值，就是交叉熵的公式了
- mapreduce原理
     - map和reduce两个过程：分而治之
- 共线性的特征会对模型产生怎样的影响？
    -  LR中特征强相关，不会影响最优性，但是会造成权重的数值解不稳定性（即模型系数每次都不一样）
- 朴素贝叶斯公式，先验概率，后验概率，条件概率
    - 贝叶斯公式：`$P\left(Y | X\right)=\frac{P\left(X | Y\right) P\left(Y\right)}{ P(Y)}$`
- 各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。
    - https://www.zhihu.com/question/26726794 
- 如何解决L1不可导问题？
    - L1能产生稀疏解，而且稀疏解的泛化能力比较好
    - subgradient: 绝对值函数只有在零点是不可导的，可以把abs的导数定义成符号函数sgn
    - proximal gradient：
- L0，L1，L2正则化
    - L0正则化的值是模型参数中非零参数的个数，L0很难优化求解是NP难问题
    - L1对应拉普拉斯分布
    - L2对应高斯分布
- 哪些常用的分类器是有VC维的，怎么计算？
    - 线性分类器： d+1
    - 高斯核分类器： 无穷
    - 神经网络：参数数量
    - 决策树：节点数+1
- 特征选择方法？
    - 皮尔森相关系数：
        - 协方差和标准差比值：`$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}$`
        - 衡量两个变量之间的线性关系，取值[-1,1]，对非线性有明显缺陷
    - 卡方检验
        - 表示自变量对应变量的相关性：`$\chi^{2}=\sum \frac{(A-E)^{2}}{E}$`
    - 互信息
        - `$I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$`
    - 基于惩罚项的特征选择
        -  L1正则
    - 基于学习模型的特征排序
        - RF，GBDT，xgboost
- 最大似然估计(MLE)VS最大后验估计(MAP)
    - MLE： `$\hat{\theta}_{\mathrm{MLE}}=\arg \max P(X |  \theta)$`，但由于连乘会造成浮点下溢，通常使用最大化对数形式
    - MAP: `$\hat{\theta}_{\mathrm{MAP}}=\underset{\theta}{\operatorname{argmax}} P(\theta | X)=\underset{\theta}{\operatorname{argmax}} \frac{P(X | \theta) P(\theta)}{P(X)} \propto \operatorname{argmax}_{\theta} P(X | \theta) P(\theta)$`
    - MLE是频率派的思想，认为参数`$\theta$`是固定的；而MAP是贝叶斯派的思想，认为参数符合某种概率分布（先验概率）
    - MLE可以认为是经验风险最小化，MAP可以认为是结构风险最小化
- 判别式模型 VS 生成式模型？
    - 判别式，无向图，求解的是条件概率，如LR,SVM,NN,CRF等
    - 生成式，有向图，求解的是联合概率，如HMM,NB,LDA等
    - 由生成式模型可以得到判别式模型，但反之不行
- 模型融合？原理？怎么选融合的模型？
    -  模型融合的方法有：voting/averaging/bagging/boosting/stacking等
    -  stacking融合：交叉验证+拼接；https://blog.csdn.net/u011630575/article/details/81302994
    -  融合模型要求： 好而不同。要求模型效果优秀且各模型个体之间尽量不同（如模型类型，模型超参数等）

## ==深度学习==

- 深度学习神经网络为什么不用牛顿法而使用梯度下降？
    - 数据量大： 牛顿法所需统计量（梯度Hessian矩阵等）不可能每一次迭代都使用全部样本
    - 维度高：参数多，导致hessian矩阵巨大
    - 非凸性：牛顿法会受限于鞍点。sgd一定会收敛，但牛顿法不一定会收敛
- Batch-norm层的作用？
    - 使网络中每层输入数据的分布相对稳定，加速模型学习速度
    - 使模型对网络中的参数不那么敏感，网络学习更稳定
    - 缓解梯度消失问题
    - 具有一定的正则化效果
- 残差网络的作用？
    - 解决深度网络的退化问题
    - 解决梯度弥散问题
    - 
- ==网络初始化有哪些方式，他们的公式 初始化过程？==
- ==优化方法 SGD、Adam算法过程 动量算法过程==
- 

## ==NLP==

- ==word2vec原理==
- glove原理？模型推导？
    - 思路是将全局词-词共现矩阵进行分解，训练得到词向量
    - 目标函数： `$J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}$`
    - `$X_{ij}$`表示单词j出现在单词i上下文中的次数
- fasttext原理
    - 模型框架类似于Word2vec的CBOW,但是也有区别：
        - 输入：word2vec用的是上下文单词的one-hot编码；fasttext用的是一个sentence的单词向量同时包括subword, 字符级别n-gran向量
        - 中间层：word2vec使用的是求和；fasttext使用的是平均；
        - 输出：cbow输出目标词汇；fasttext输出文本分类标签
    - 速度快，使用层次softmax，对oov友好 
    - 有监督的文本分裂，无监督的词向量学习
- glove和word2vec区别：
    -  g是基于全局语料的，w是基于滑动窗口局部语料的；因此w可以进行在线学习，而g则需要固定语料信息
    -  目标函数不一样
    -  g更快，对于高频词的处理更有效
- LDA的词表示和word2vec的词表示有什么区别？
    - LDA出来的词向量不如word2vec的精细，一些任务无法完成
    - 训练过程LDA使用的是狄利克雷先验分布，属于生成式模型；word2vec使用的是语言模型，属于判别式模型
    - 
- 分词的原理
    - 最大正向匹配，最大逆向匹配，双向匹配
    - n-gram
    - BMES模型（HMM，CRF）
- 字典树的优缺点？手写字典树？
    - 优点：查询速度快，可以做前缀比较
    - 缺点：内存消耗大
- 输入补全可以由那种数据结构来做？
    - 字典树 
- 为什么正则化处理中bias不需要正则
    - bias影响的是结果的偏差，对输入没有放大缩小的作用，而正则化要降低方差（提高模型泛化能力），所以不需要正则化bias
- 反卷积与反池化
    - 反卷积：
    - 反池化：
- attention机制原理
- transformer结构图以及原理
- BLEU原理及代码
- dropout train和test的区别
    - train过程：输出会依一定概率p让输入为0
    - test过程： 需要乘上概率p
- BN  train和test的区别？
    - train： 有batch
    - test：只有单个样本，所以方差和均值使用之前训练阶段每个batch保存下来的平均
- 正负样本不平衡问题？
    -  过采样或欠采样
    -  组合/集成方法：
    -  正负样本设置惩罚权重
    -  ==focal loss==
- BN前向传播公式，反向求导公式？
    -  对一批数据进行归一化，沿着特征方向：`$B N\left(x_{i}\right)=\alpha \times \frac{x_{i}-\mu_{b}}{\sqrt{\sigma_{B}^{2}+\epsilon}}+\beta$`
    -  其中两个参数alpha和beta是调节参数，可学习的，防止网络表达能力下降
    -  https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
- BN为什么不适用与RNN？
    - RNN是变长的，基于timestep计算，如果使用BN，需要保存每个timestep下batch的均值和方差，效率很低 
- 加了dropout后，神经网络的BP阶段有什么改变，求导公式要怎么改
    - 需要乘上mask向量，因为在前向传播过程中失活的神经元在反向传播时应该也是要不起作用的
- 试分析为什么不能在循环神经网络中的循环连接上直接应用丢弃法？
    - 若放在循环链接，则信息将会因循环进行而逐渐丢失
    - rnn的dropout只能设置在输入输出层上
- 在LSTM中，隐藏层神经元数量与参数数量之间的关系？
    -  对照LSTM中gate过程的公式可算出：num=4*（O*(I+H)+O）
- ==CNN卷积的反向传播过程==
- ==胶囊网络？==
    - CNN对物体之间的空间关系识别能力不强
    - CNN对物体旋转之后识别能力不强
    - 胶囊是一个包含多个神经元的载体，每个神经元表示了图像中出现的特定实体的各种属性
    - https://www.tinymind.cn/articles/61
- 转置卷积？
    - 一般卷积：`$\mathbf{z}=C \mathbf{x}$`
    - 转置卷积: `$\mathbf{x}=C^{\mathrm{T}} \mathbf{z}$`
- 微步卷积？
    - 步长s<1的转置卷积 
    - 如果卷积操作的步长s>1，希望其对应的转置卷积的步长为1/s，需要在输入特征之间插入s-1个0来使得其移动的速度变慢
- 空洞卷积？存在什么问题？
    - 通过间隔地对输入进行卷积操作以增加输出单元的感受野
- 1*1卷积核的作用？
    - 降维
    - 升维
    - 增加非线性
- ==TensorFlow训练模型的整个流程？==
- ==梯度消失是否可以通过增加学习率来缓解？==
- 卷积层神经元数量计算？
    - 假设卷积层的输入神经元个数为n，卷积大小为m，步长（stride）为s，输入神经元两端各填补p 个零（zero padding），则神经元数量为`$(n-m+2 p) / s+1$`
- ==CNN的反向传播？==
- 什么样的任务适合用深度学习，什么样的问题不适合？
    - 不适合：
        - 小样本
        - 低维数据，大样本量，不如ensemble
        - 
- NLP中的CNN的max pooling
    - 能减少模型参数数量，有利于缓解过拟合
    - 可以把变长的输入X整理成固定长度的输入
    - 丢失最大强度特征的位置信息
    - 有时候强特征出现多次
    - 改进：k-max pooling和chunk-max pooling
- LSTM中为什么选择tanh？
    - https://www.zhihu.com/question/61265076
- 算法的错误样例分析方法？（即预测与真实标签不符）
- warmup策略为什么有效？
    -  warmup是指在训练初始阶段使用较小的学习率来启动，接着切换到大学效率而后进行常见的decay训练
    -  有助于缓解模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳
    -  有助于保持模型深层的稳定性

## ==Python语言相关！！！==
- python内存机制
    - 引用计数，垃圾回收，内存池 
    - https://www.cnblogs.com/geaozhang/p/7111961.html 
- python中dictionary，set的底层原理 
    - 使用伪随机探测的散列表作为字典的底层数据结构
- 介绍一下构造函数，析构函数，函数重载(面向对象这一块的知识)
    - `__metaclass__` 元类方法，在创建类的时候执行 
    - `__new__` 是一个静态方法，返回一个创建的实例
    - `__init__`构造函数，在初始化实例的时候会执行，没有返回
    - `__del__`析构函数，被python垃圾回收器销毁的时候调用
    - python中不需要函数重载
- 解释python元类？
    - 首先理解python中的类也是对象
    - 元类就是用来创建类的（type()就是python在背后用来创建所有类的元类）
- python的静态方法，类方法和实例方法？
    - 实例方法就是一般的方法，需要先将类实例化后调用，关键字`self`
    - 静态方法(staticmethod)不需要创建类的实例就可以调用，没有关键字
    - 类方法(classmethod)可以使用类和实例调用，关键字`cls`
- python自省？
    -  自省就是运行时能知道对象的类型，比如type(), dir(), getattr(), hasattr(), isinstance()
-  新式类和旧式类？
    - https://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html 
-  python里的拷贝（引用，copy(),deepcopy()）
    - 引用， `b=a`，a改变b也改变
    - copy(), 浅拷贝，拷贝父对象，但不会拷贝对象的内部子对象
    - deepcopy(), 深拷贝，完全拷贝父对象及其子对象
- python的is和=？
    - `is`是对比地址
    - `=`是对比值
- python如何实现单例模式?请写出两种实现方式?
- python中变量查找顺序？
    - LEGB顺序，即
        - local， 函数内部作用域
        - enclosing， 函数内部与内嵌函数之间
        - global， 全局作用域
        - build-in， 内置作用域
- 请描述抽象类和接口类的区别和联系？
- python中的进程和线程
    -  进程：一个运行的程序（代码）就是一个进程，没有运行的代码叫程序，进程是系统资源分配的最小单位，进程拥有自己独立的内存空间，所有进程间数据不共享，开销大
    -  线程: cpu调度执行的最小单位，也叫执行路径，不能独立存在，依赖进程存在，一个进程至少有一个线程，叫主线程，而多个线程共享内存（数据共享，共享全局变量),从而极大地提高了程序的运行效率。
    -  协程: 是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操中栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。


## 寿司代码 & 场景题！！！==
- 排序啊查找啊基础
- 千万向量中找到和单个向量相似的那个  
    - 先聚类，然后输入向量先与聚类中心比较再与类中的向量比较
- 海量数据排序（找中位数）
    -  https://blog.csdn.net/sykpour/article/details/26480217
- 判断是否平衡二叉树
    - 设置flag，一边遍历一遍记录深度，如果深度差大于1，则flag设置为False 
- 扇形涂色问题。一个圆被分成M个扇形，一共有N种颜色，相邻扇形不同色，一共有几种涂法？
- 判断二叉树对称
- 递归和非递归实现求二叉树深度
- 求字符串中最长的不重复子串的长度（双指针、dp）
- 大数加法？大数乘法？
- 给个一个二维矩阵，矩阵中每一位是字符，从任意一个字符出发，不断向上下左右能拼成一个字符串；给定一些字符串，逐个判断是否能在该矩阵中拼出
- 一棵二叉树，每个节点的值是一个整数，求这个树中路径上数字都相同的路径中最长的一条
- 求n的m次方根，怎么转换成优化算法然后解
- 求树两个节点的最低公共祖先
- 实现一个hashmap，包含put get delete方法
-  给定数组{a1, a2, a3, ... an}，要求挑出一些数，这些数不能相邻，使得加起来的和最大。如果是环怎么处理？
- 一个四位数abcd，满足abcd*4=dcba,求这个数？
- 一个链表，奇数位升序，偶数为降序，将其转化成完全升序的链表
- 求逆序对的数目
- 求二维矩阵的子矩阵，使得和最大
- 名人问题，给出最优解？
- 01矩阵找出由1组成的最大面积的矩阵？
- 编辑距离
- 射气球
- 求二叉树哪一个点到其他所有点的路径总和最小，即树的重心？
- 设计一个结构存取稀疏矩阵
    - 只存非零元素，三元组形式：（元素，行，列） 
- 电梯上升只能停一层，大家下电梯再各自步行到自己的楼层，求停在哪一层所有人步行层数总和最少？
- 给一个环形数组长度为N，数组每个位置都有一种颜色，总共M种颜色，求包含全部颜色的最短长度
- 有M个有序链表（从大到小），要求取出前K大的元素。
    - 把M个链表的头结点做成一个大小为M的最大堆，每次取出堆中最大的节点，然后将这个节点的后续节点放进堆中重新排序
    - 时间复杂度`$O((k+m)log^{m})$`，空间复杂度`$O(M)$`
- 求A的长度为L的各个连续子串在B中出现的次数总和
    - 先预处理一下字符串B，将B中长度为L的子串及其出现次数记录在hashmap里；然后去遍历A中长度为L的子串并查找hashmap；时间复杂度O(N)
- N！中0的个数？
    -  关注5的个数
- 二叉树中是否存在和为target的路径？
    - dfs
- ==B+、 B、 平衡二叉树 、AVL树、红黑树的区别==
- 实现带有求最大值功能的栈，要求时间复杂度为O（1）
    - 两个栈，一个保存数据，一个保存每次操作后的最大元素 
- 给出一个n*n数字矩阵，寻找一条最长上升路径，每个位置只能向上下左右四个位置移动。
- 环形打印二维矩阵？
- 求二维数组中，任意矩形之和？
